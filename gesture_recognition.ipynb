{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We set the random seed value in order to maintain the reproducibility of the network and also for results not to vary drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we store training and validation folders in separate lists and shuffle them so that our model should not overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open(\"D:/ML&AI/Gesture-Recognition-with-3D-CNN-and-CNN-RNN/train.csv\").readlines())\n",
    "val_doc = np.random.permutation(open(\"D:/ML&AI/Gesture-Recognition-with-3D-CNN-and-CNN-RNN/val.csv\").readlines())\n",
    "batch_size = 64  # We have to experiment with it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WIN_20180926_16_54_08_Pro_Right_Swipe_new;Right_Swipe_new;1\\n'\n",
      " 'WIN_20180925_18_02_58_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n'\n",
      " 'WIN_20180925_17_33_08_Pro_Left_Swipe_new;Left_Swipe_new;0\\n'\n",
      " 'WIN_20180925_17_51_17_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n'\n",
      " 'WIN_20180926_17_17_35_Pro_Left_Swipe_new;Left_Swipe_new;0\\n']\n"
     ]
    }
   ],
   "source": [
    "print(train_doc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying constants\n",
    "num_classes = 5  # total no of classes\n",
    "ht = 84         # height of image (desired)\n",
    "wd = 84         # width of image (desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "- This is the most important part of the code\n",
    "- Data Generators help to utilise the system memory efficiently by feeding the data into batches\n",
    "- We should preprocess the data before feeding into the CNN\n",
    "- Preprocessing including cropping, resizing, normalizing the data, etc\n",
    "- Videos captured from different cameras might have different dimensions(resolution) and that needs to be taken care of with image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Frames: 18\n"
     ]
    }
   ],
   "source": [
    "tot_frames = len([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29])\n",
    "print(\"Total Frames:\", tot_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this generator\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size,tot_frames,ht,wd,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = resize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,tot_frames,ht,wd,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = resize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences(videos): 663\n",
      "# validation sequences(videos): 100\n",
      "# no of epochs: 30\n"
     ]
    }
   ],
   "source": [
    "current_dt_time = datetime.datetime.now()\n",
    "train_path = \"D:/ML&AI/Gesture-Recognition-with-3D-CNN-and-CNN-RNN/project_data/train\"\n",
    "val_path = \"D:/ML&AI/Gesture-Recognition-with-3D-CNN-and-CNN-RNN/project_data/val\"\n",
    "\n",
    "train_videos = len(train_doc)\n",
    "val_videos = len(val_doc)\n",
    "print(\"# training sequences(videos):\", train_videos)\n",
    "print('# validation sequences(videos):', val_videos)\n",
    "\n",
    "num_epochs = 30\n",
    "print('# no of epochs:', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "Here we are using 3D Convolution Neural Network. So we should use `Conv3D` and `MaxPooling3D` layers instead of `Conv2D` and `MaxPooling2D` layers\\\n",
    "Last layer is `softmax` and model should be built in such a way that it should be able to fit in the memory of webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, Flatten, Dense, MaxPool3D, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model without `BatchNormalization` and `Dropout` and activation function `ReLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "callbacks_list = [LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  D:/ML&AI/Gesture-Recognition-with-3D-CNN-and-CNN-RNN/project_data/train ; batch size = 64\n",
      "Epoch 1/15\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.7088 - categorical_accuracy: 0.1825 Source path =  D:/ML&AI/Gesture-Recognition-with-3D-CNN-and-CNN-RNN/project_data/val ; batch size = 64\n",
      "11/11 [==============================] - 167s 15s/step - loss: 1.7088 - categorical_accuracy: 0.1825 - val_loss: 1.5733 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "11/11 [==============================] - 162s 15s/step - loss: 1.5507 - categorical_accuracy: 0.2474 - val_loss: 1.5024 - val_categorical_accuracy: 0.3800 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "11/11 [==============================] - 154s 14s/step - loss: 1.3615 - categorical_accuracy: 0.3861 - val_loss: 1.2051 - val_categorical_accuracy: 0.5800 - lr: 0.0010\n",
      "Epoch 4/15\n",
      " 1/11 [=>............................] - ETA: 2:17 - loss: 1.1919 - categorical_accuracy: 0.4844"
     ]
    }
   ],
   "source": [
    "conv3d_model1 = Sequential([\n",
    "    Conv3D(16, (3,3,3),padding='same', input_shape=(tot_frames,ht,wd,3), activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(32, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(64, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "# COMPILING THE MODEL\n",
    "conv3d_model1.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "# print(conv3d_model1.summary())\n",
    "\n",
    "# GENERATORS\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "    \n",
    "history = conv3d_model1.fit(train_generator,\n",
    "          epochs=15,  # keeping no of epochs as 5 for now\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator,steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)\n",
    "print(history)\n",
    "# print(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(15)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model1.save('model-1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model is able to \"learn\" on training dataset very well. But we have slight overfitting on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model with `BatchNormalization` and `Dropout` to try reduce the overfitting\\\n",
    "Also increasing the batch size to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model2 = Sequential([\n",
    "    Conv3D(16, (3,3,3),padding='same', input_shape=(tot_frames,ht,wd,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(32, (3,3,3),padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(64, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.25),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "conv3d_model2.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(conv3d_model2.summary())\n",
    "\n",
    "batch_size = 128\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "# CREATING GENERATOR OBJECTS\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = conv3d_model2.fit(train_generator,\n",
    "          epochs=15,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(15)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model2.save('model-2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus it is evident that batch size = 128 is not optimal choice\\\n",
    "We should use batch size = 64\\\n",
    "Because model is overfitting. The probable reason for this can be batch size = 128 might not be optimal for validation set though it is good for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "Trying model similar to the `conv3d_model1` i.e **Model1**. But this time we will try reducing the parameters by reducing no of neurons in the last hidden layer and keeping `batch_size = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model3 = Sequential([\n",
    "    Conv3D(16, (3,3,3),padding='same', input_shape=(tot_frames,ht,wd,3), activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(32, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(64, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),  # Last hidden layer neurons reduced to 64\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "conv3d_model3.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(conv3d_model3.summary())\n",
    "\n",
    "# SETTING THE BATCH SIZE\n",
    "batch_size = 64\n",
    "\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "# CREATING GENERATOR OBJECTS\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = conv3d_model3.fit(train_generator,\n",
    "          epochs=20,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(20)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model3.save('model-3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences:\n",
    "- **Model 3** has least no of parameters as compared to Model 1 and Model 2\n",
    "- Accuracy of model 3 is similar to that of model 1 and that too with half the parameters of model 1\n",
    "- It makes model 3 the best choice for 3D CNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "- Trying with batch size = 32 with same model as model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model4 = Sequential([\n",
    "    Conv3D(16, (3,3,3),padding='same', input_shape=(tot_frames,ht,wd,3), activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(32, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(64, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),  # Last hidden layer neurons reduced to 64\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "conv3d_model4.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(conv3d_model4.summary())\n",
    "\n",
    "# SETTING THE BATCH SIZE\n",
    "batch_size = 32\n",
    "\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "# CREATING GENERATOR OBJECTS\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = conv3d_model4.fit(train_generator,\n",
    "          epochs=15,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(15)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model4.save('model-4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences\n",
    "- Batch size = 32 turns out to be best batch size\n",
    "- Gives the least overfit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets do some modifications in the model\n",
    "> Adding 1 more hidden layers before output layer in previous model\\\n",
    "> Batch size = 32\\\n",
    "> Using dropouts = 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model5 = Sequential([\n",
    "    Conv3D(16, (3,3,3),padding='same', input_shape=(tot_frames,ht,wd,3), activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(32, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(64, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Conv3D(128, (3,3,3),padding='same', activation='relu'),\n",
    "    MaxPool3D(pool_size=(2,2,2), strides=(2,2,2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.25),\n",
    "    Dense(64, activation='relu'),  # Last hidden layer neurons reduced to 64\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "conv3d_model5.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(conv3d_model5.summary())\n",
    "\n",
    "# SETTING THE BATCH SIZE\n",
    "batch_size = 32\n",
    "\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "# CREATING GENERATOR OBJECTS\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = conv3d_model5.fit(train_generator,\n",
    "          epochs=15,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(15)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_model5.save('model-5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6\n",
    "- In this model, we convert images to grayscale from RGB\n",
    "- This will help us get results faster as memory consumed for each batch will be reduced\n",
    "- We have to modify our existing `data generator` for converting images to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_grayscale(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size,tot_frames,ht,wd))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = rgb2gray(imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32))\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = resize(image[:,20:140],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:] = image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,tot_frames,ht,wd))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = rgb2gray(imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32))\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = resize(image[:,20:140],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:] = image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_model6 = Sequential([\n",
    "    Conv2D(16, (3,3),padding='same', input_shape=(tot_frames,ht,wd), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(32, (3,3),padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(64, (3,3),padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "conv2d_model6.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(conv2d_model6.summary())\n",
    "\n",
    "# SETTING THE BATCH SIZE\n",
    "batch_size = 64\n",
    "\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "# CREATING GENERATOR OBJECTS\n",
    "train_generator = generator_grayscale(train_path,train_doc,batch_size)\n",
    "val_generator = generator_grayscale(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = conv2d_model6.fit(train_generator,\n",
    "          epochs=20,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(20)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_model6.save('model-6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7\n",
    "- The model is overfitting after 15 iterations\n",
    "- We will use *Dropout* to tackle overfitting\n",
    "- Also, will increase neurons in last hidden layer to push training accuracy furthur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_model7 = Sequential([\n",
    "    Conv2D(16, (3,3),padding='same', input_shape=(tot_frames,ht,wd), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(32, (3,3),padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(64, (3,3),padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "conv2d_model7.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(conv2d_model7.summary())\n",
    "\n",
    "# SETTING THE BATCH SIZE\n",
    "batch_size = 64\n",
    "\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "# CREATING GENERATOR OBJECTS\n",
    "train_generator = generator_grayscale(train_path,train_doc,batch_size)\n",
    "val_generator = generator_grayscale(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = conv2d_model7.fit(train_generator,\n",
    "          epochs=20,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(20)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_model7.save('model-7.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences:\n",
    "- Grayscale model doesn't perform well as it results in lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now trying CNN-RNN Architecture\n",
    "- Using transfer learning for 2D CNN layer\n",
    "- Stacking GRU model on top of it\n",
    "\n",
    "*Note:* Here we prefer to use GRU instead of LSTM as they both give similar accuracy and GRU has 3 gates while LSTM has 4, which reduces no of parameters for GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, Flatten, TimeDistributed, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using ResNet50 pre-trained model for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(include_top=False, weights='imagenet',input_shape=(ht,wd,3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "features = Dense(64, activation='relu')(x)\n",
    "conv_model = Model(inputs=base_model.input, outputs=features)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# CNN-RNN MODEL 1\n",
    "cnn_rnn_m1 = Sequential([\n",
    "    TimeDistributed(conv_model, input_shape=(tot_frames,ht,wd,3)),\n",
    "    GRU(160, return_sequences=True),\n",
    "    GRU(320),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# COMPILING THE MODEL\n",
    "cnn_rnn_m1.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['categorical_accuracy'])\n",
    "print(cnn_rnn_m1.summary())\n",
    "\n",
    "batch_size = 64\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = cnn_rnn_m1.fit(train_generator,\n",
    "          epochs=15, # keeping 5 epochs for now\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(15)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_m1.save('model-8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasing last hidden layer neurons in pre-trained model (ResNet50) to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(include_top=False, weights='imagenet',input_shape=(ht,wd,3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "features = Dense(128, activation='relu')(x)  # here we have changed neurons\n",
    "conv_model = Model(inputs=base_model.input, outputs=features)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# CNN-RNN MODEL 1\n",
    "cnn_rnn_m2 = Sequential([\n",
    "    TimeDistributed(conv_model, input_shape=(tot_frames,ht,wd,3)),\n",
    "    GRU(160, return_sequences=True),\n",
    "    GRU(320),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# COMPILING THE MODEL\n",
    "cnn_rnn_m2.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['categorical_accuracy'])\n",
    "print(cnn_rnn_m2.summary())\n",
    "\n",
    "batch_size = 64\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = cnn_rnn_m2.fit(train_generator,\n",
    "          epochs=15,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(15)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this model also, results are similar to the first model\n",
    "- No significant change was observed\n",
    "- Let's switch our model to ResNet50V2 which is advanced version of ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_m2.save('model-9.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using ResNet50V2 pre-trained model for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size = 64\n",
    "- Last hidden layer neurons in ResNet50V2 = 64\n",
    "- epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50V2(include_top=False, weights='imagenet',input_shape=(ht,wd,3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "features = Dense(64, activation='relu')(x)\n",
    "conv_model = Model(inputs=base_model.input, outputs=features)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# CNN-RNN MODEL 3\n",
    "cnn_rnn_m3 = Sequential([\n",
    "    TimeDistributed(conv_model, input_shape=(tot_frames,ht,wd,3)),\n",
    "    GRU(160, return_sequences=True),\n",
    "    GRU(320),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# COMPILING THE MODEL\n",
    "cnn_rnn_m3.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['categorical_accuracy'])\n",
    "print(cnn_rnn_m3.summary())\n",
    "\n",
    "batch_size = 64\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = cnn_rnn_m3.fit(train_generator,\n",
    "          epochs=10,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(10)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_m3.save('model-10.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the best model so far in the CNN-RNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trying LSTM instead of GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50V2(include_top=False, weights='imagenet',input_shape=(ht,wd,3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "features = Dense(128, activation='relu')(x)\n",
    "conv_model = Model(inputs=base_model.input, outputs=features)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# CNN-RNN MODEL 3\n",
    "cnn_rnn_m5 = Sequential([\n",
    "    TimeDistributed(conv_model, input_shape=(tot_frames,ht,wd,3)),\n",
    "    LSTM(160, return_sequences=True),\n",
    "    LSTM(320),\n",
    "    Dropout(0.25),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(256, activation='relu'),   # Adding this new layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# COMPILING THE MODEL\n",
    "cnn_rnn_m5.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['categorical_accuracy'])\n",
    "print(cnn_rnn_m5.summary())\n",
    "\n",
    "batch_size = 64\n",
    "# For training set\n",
    "if (train_videos%batch_size) == 0:\n",
    "    steps_per_epoch = int(train_videos/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(train_videos//batch_size) + 1\n",
    "    \n",
    "# For validation set\n",
    "if (val_videos%batch_size) == 0:\n",
    "    validation_steps = int(val_videos/batch_size)\n",
    "else:\n",
    "    validation_steps = int(val_videos//batch_size) + 1\n",
    "\n",
    "train_generator = generator(train_path,train_doc,batch_size)\n",
    "val_generator = generator(val_path,val_doc,batch_size)\n",
    "\n",
    "# FITTING THE MODEL\n",
    "history = cnn_rnn_m5.fit(train_generator,\n",
    "          epochs=5,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=val_generator, steps_per_epoch=steps_per_epoch,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(5)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_m5.save('model-11.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
